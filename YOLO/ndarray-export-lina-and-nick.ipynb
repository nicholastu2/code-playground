{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get voronoi boundaries\n",
    "\n",
    "# get numpy array of image\n",
    "#   - get centers of nuclei in numpy array\n",
    "#   - prediction.boxes.xywh.numpy()\n",
    "#       - export to csv\n",
    "#           - how do you export to csv?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision:  0.17.2\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import socket\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from random import randint\n",
    "import re\n",
    "import imagecodecs\n",
    "import tifffile\n",
    "import tensorflow as tf\n",
    "from scipy.spatial import Voronoi, cKDTree\n",
    "from matplotlib.path import Path\n",
    "from rtree import index\n",
    "from ctypes.wintypes import INT\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, Point\n",
    "import torch, torchvision\n",
    "\n",
    "print(\"torchvision: \", torchvision.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model_path = r\"S:\\Phys\\FIV925 XSection\\Datasets\\Axons vs SCs\\01i5\\YO 384 0530 MAXI\\map75=0855192 YOLOv8n-seg map75=081193 idx=4 ep=24 btch=32 rnd=2840511\\weights\\best.pt\"\n",
    "\n",
    "model = YOLO(yolo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FIVE\\anaconda3\\envs\\torch01\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x384 (no detections), 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 16.4ms postprocess per image at shape (1, 3, 384, 384)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tile_size = 384\n",
    "iteration = 0\n",
    "scale_factor = 1\n",
    "dataset = \"Human Lung Cancer\"\n",
    "base_path = r\"R:\\FIVE\\EXP\\FIV925\\Additional Datasets\"\n",
    "ome_tiff_path = f\"{base_path}\\\\{dataset}\\\\morphology_mip.ome.tif\"\n",
    "\n",
    "with tifffile.TiffFile(ome_tiff_path) as tif:\n",
    "    image = tif.asarray()\n",
    "    # image_dapi = image[2,:, :] # DAPI channel\n",
    "    image_data = image[::2, ::2] # downsample by half\n",
    "\n",
    "    # normalize to 8 bit\n",
    "    max_val_16bit = np.max(image_data)\n",
    "    image_scaled = (image_data / max_val_16bit) * 255\n",
    "    image_8bit = image_scaled.astype(np.uint8)\n",
    "\n",
    "    # create image and resize\n",
    "    og_image = Image.fromarray(image_8bit, 'L')\n",
    "    new_width = int(og_image.width * scale_factor); new_height = int(og_image.height * scale_factor)\n",
    "    image_resized = og_image.resize((new_width, new_height), Image.LANCZOS)\n",
    "    image = np.array(image_resized)\n",
    "\n",
    "    image_height, image_width = image_resized.size\n",
    "\n",
    "    big_array = np.zeros(image_resized.size)\n",
    "    list_xywh = []\n",
    "\n",
    "    # as we're making predictions, we want a big array that's meant to 'contain' the whole image\n",
    "    #   then insert the predictions into that big array\n",
    "    # so here is where we go through the image as tiles and make the predictions\n",
    "    for i in range(0, image_height, 384): # section_miny, section_maxy\n",
    "        for j in range(0, image_width, 384): # section_minx, section_maxx\n",
    "            tile = image[i:i+tile_size, j:j+tile_size]\n",
    "            \n",
    "            if tile.shape[0] == 0 or tile.shape[1] == 0:\n",
    "                continue  # Skip tiles that have no width or height\n",
    "            tile_as_img = Image.fromarray(tile, 'L')\n",
    "            prediction = model.predict(tile_as_img, show=False, max_det=3000)\n",
    "            \n",
    "            # trying to make the numpy array with the centers of the boxes\n",
    "            #   theoretically, this should a numpy array of size 384x384 (in most cases)\n",
    "            #   wrong, this is a list of the xywh coords\n",
    "            # small_array = prediction[0].boxes.xywh.cpu().numpy()\n",
    "            # big_array[i:i+small_array.shape[0], j:j+small_array.shape[1]] = small_array\n",
    "            list_xywh.append(prediction[0].boxes.xywh.cpu().numpy())\n",
    "            iteration += 1\n",
    "            print(iteration)\n",
    "    \n",
    "            points = list_xywh\n",
    "            \n",
    "            # from Lina's code\n",
    "            # if len(points) < 4:\n",
    "            #     continue\n",
    "            if len(set(points[:, 0])) > 1 and len(set(points[:, 1])) > 1:\n",
    "                vor = Voronoi(points)\n",
    "            else:\n",
    "                continue\n",
    "            region_paths = {}\n",
    "            transcript_voronoi_regions = []\n",
    "\n",
    "            for point_region_id, region_id in enumerate(vor.point_region):\n",
    "                if (-1 not in vor.regions[region_id]):\n",
    "                    region_paths[point_region_id] = Path(vor.vertices[vor.regions[region_id]])\n",
    "            region_paths = {}\n",
    "            voronoi_regions = []\n",
    "\n",
    "            for point_region_id, region_id in enumerate(vor.point_region):\n",
    "                if (-1 not in vor.regions[region_id]):\n",
    "                    region_paths[point_region_id] = Path(vor.vertices[vor.regions[region_id]])\n",
    "\n",
    "            ## For each transcript point, identify which Voronoi cell it falls into\n",
    "            for point in points:\n",
    "                point_region_id = -1  # Default to None if no region contains the point\n",
    "                for region_id, region_path in region_paths.items():\n",
    "                    if region_path.contains_point(point):\n",
    "                        point_region_id = region_id\n",
    "                        break  # Stop checking if we found the region\n",
    "                voronoi_regions.append(point_region_id)\n",
    "            # end of Lina's code    \n",
    "            \n",
    "            # big_array[i:i+tile.shape[0], j:j+tile.shape[1]] = voronoi_regions\n",
    "                        \n",
    "# Save the array with a header and formatted floating-point numbers\n",
    "# np.savetxt(r\"R:\\FIVE\\EXP\\FIV925\\Additional Datasets\\Human Lung Cancer\\array.csv\", big_array, delimiter=\",\", fmt='%.2f')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = prediction[0].boxes.xywh.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 35.660927  268.86853    13.50577    12.989868 ]\n",
      " [ 23.839218  276.08948    13.931564   13.951111 ]\n",
      " [100.46495   182.72694    10.622742   11.629486 ]\n",
      " ...\n",
      " [280.11145   130.82216    12.093018    9.311768 ]\n",
      " [  2.3115494 364.06012     4.5220504   9.036438 ]\n",
      " [237.07748    78.93929     9.006226   12.2500305]]\n"
     ]
    }
   ],
   "source": [
    "print(ar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
